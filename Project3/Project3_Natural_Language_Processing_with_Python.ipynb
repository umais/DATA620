{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Title: Project 3 - Natural Language Processing with Python**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Submitted by:** Umais Siddiqui, Banu Boopalan\n",
        "\n",
        "**Date:** March 31st, 2025\n",
        "\n",
        "**Course:** Data Science – DATA620\n",
        "\n",
        "**Video Link:**\n",
        "\n",
        "**Github Repository:** https://github.com/umais/DATA620/blob/master/Project3/Project3_Natural_Language_Processing_with_Python.ipynb\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FLSb1scOCiqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Introduction**\n",
        "\n",
        "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can. Begin by splitting the Names Corpus into three subsets:\n",
        "\n",
        "- 500 words for the test set\n",
        "- 500 words for the dev-test set\n",
        "- The remaining 6900 words for the training set.\n",
        "\n",
        "Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set.\n",
        "\n",
        "How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?\n",
        "\n",
        "**Source:** Natural Language Processing with Python, exercise 6.10.2."
      ],
      "metadata": {
        "id": "uUk_MjrKkBTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Loading the Data**\n",
        "\n",
        "In this project we take the names corpus from nltk.corpus.names, which contains lists of male and female names.The dataset is shuffled to ensure randomness."
      ],
      "metadata": {
        "id": "LTi1T-S8dXXi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjKIMZAbCcw7",
        "outputId": "b035e4af-2474-4627-dfaf-3d470cad404f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import names\n",
        "from nltk.classify import apply_features\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify import DecisionTreeClassifier, MaxentClassifier\n",
        "nltk.download('names')\n",
        "\n",
        "# Load and shuffle the labeled names dataset\n",
        "labeled_names = [(name, 'male') for name in names.words('male.txt')] + \\\n",
        "                [(name, 'female') for name in names.words('female.txt')]\n",
        "random.shuffle(labeled_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Splitting the Data**\n",
        "\n",
        "The dataset is divided into:\n",
        "\n",
        "- Training set (6900 names) → used to train the classifier.\n",
        "\n",
        "- Development-test (dev-test) set (500 names) → used to tune and evaluate improvements.\n",
        "\n",
        "- Test set (500 names) → used for final performance evaluation."
      ],
      "metadata": {
        "id": "CmlQj66idquq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training, dev-test, and test sets\n",
        "train_names = labeled_names[1000:]\n",
        "dev_test_names = labeled_names[500:1000]\n",
        "test_names = labeled_names[:500]\n",
        "\n"
      ],
      "metadata": {
        "id": "xehjjLAdeHwU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Engineering**\n",
        "\n",
        "Instead of only using the last letter of a name, we extract:\n",
        "\n",
        "- Last letter\n",
        "- Last two letters\n",
        "- Last three letters\n",
        "- First letter\n",
        "- Length of the name\n",
        "- Vowel count"
      ],
      "metadata": {
        "id": "slJUW4aDeZWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gender_features(name):\n",
        "    return {\n",
        "        \"last_letter\": name[-1],\n",
        "        \"last_two\": name[-2:],\n",
        "        \"last_three\": name[-3:],\n",
        "        \"first_letter\": name[0],\n",
        "        \"length\": len(name),\n",
        "        \"vowel_count\": sum(1 for char in name.lower() if char in 'aeiou')\n",
        "    }\n",
        "# Extract features for each dataset\n",
        "train_set = [(gender_features(n), g) for (n, g) in train_names]\n",
        "dev_test_set = [(gender_features(n), g) for (n, g) in dev_test_names]\n",
        "test_set = [(gender_features(n), g) for (n, g) in test_names]"
      ],
      "metadata": {
        "id": "iFhR2iLJem0Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Training the Model**\n",
        "\n",
        "A Naive Bayes classifier is trained on the extracted features."
      ],
      "metadata": {
        "id": "di1FW0jOereI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the classifier\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Train Naive Bayes Classifier\n",
        "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier.train(train_set)\n",
        "\n",
        "# Train Maximum Entropy Classifier (requires SciPy)\n",
        "me_classifier = MaxentClassifier.train(train_set, algorithm='GIS', trace=0, max_iter=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KagSsi_de0ms",
        "outputId": "51fedf25-9a64-4acb-e997-a75fdb5aa2da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev-Test Accuracy:\n",
            "Naive Bayes: 0.8200\n",
            "Decision Tree: 0.7360\n",
            "MaxEnt: 0.8180\n",
            "\n",
            "Test Accuracy:\n",
            "Naive Bayes: 0.8060\n",
            "Decision Tree: 0.7380\n",
            "MaxEnt: 0.8100\n",
            "Most Informative Features\n",
            "                last_two = 'na'           female : male   =     93.6 : 1.0\n",
            "                last_two = 'ia'           female : male   =     84.4 : 1.0\n",
            "                last_two = 'la'           female : male   =     69.7 : 1.0\n",
            "             last_letter = 'a'            female : male   =     37.2 : 1.0\n",
            "                last_two = 'sa'           female : male   =     34.4 : 1.0\n",
            "             last_letter = 'k'              male : female =     30.1 : 1.0\n",
            "                last_two = 'rd'             male : female =     30.0 : 1.0\n",
            "                last_two = 'ta'           female : male   =     29.5 : 1.0\n",
            "                last_two = 'us'             male : female =     27.6 : 1.0\n",
            "              last_three = 'ana'          female : male   =     24.6 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Evaluating Performance**\n",
        "\n",
        "The classifier is tested on:\n",
        "- The dev-test set (to fine-tune the model).\n",
        "- The test set (to check real-world performance).\n",
        "- The accuracy on both sets is printed.\n",
        "- The most informative features are displayed."
      ],
      "metadata": {
        "id": "njDJDOq8e-wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate each classifier on dev-test set\n",
        "print(\"Dev-Test Accuracy:\")\n",
        "print(f\"Naive Bayes: {nltk.classify.accuracy(nb_classifier, dev_test_set):.4f}\")\n",
        "print(f\"Decision Tree: {nltk.classify.accuracy(dt_classifier, dev_test_set):.4f}\")\n",
        "print(f\"MaxEnt: {nltk.classify.accuracy(me_classifier, dev_test_set):.4f}\")\n",
        "\n",
        "# Evaluate each classifier on test set\n",
        "print(\"\\nTest Accuracy:\")\n",
        "print(f\"Naive Bayes: {nltk.classify.accuracy(nb_classifier, test_set):.4f}\")\n",
        "print(f\"Decision Tree: {nltk.classify.accuracy(dt_classifier, test_set):.4f}\")\n",
        "print(f\"MaxEnt: {nltk.classify.accuracy(me_classifier, test_set):.4f}\")\n",
        "\n",
        "# Display the most informative features for Naive Bayes\n",
        "nb_classifier.show_most_informative_features(10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aba8P7ffOGF",
        "outputId": "81a31215-d42d-41d5-a64d-73683db97282"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev-Test Accuracy:\n",
            "Naive Bayes: 0.8200\n",
            "Decision Tree: 0.7360\n",
            "MaxEnt: 0.8180\n",
            "\n",
            "Test Accuracy:\n",
            "Naive Bayes: 0.8060\n",
            "Decision Tree: 0.7380\n",
            "MaxEnt: 0.8100\n",
            "Most Informative Features\n",
            "                last_two = 'na'           female : male   =     93.6 : 1.0\n",
            "                last_two = 'ia'           female : male   =     84.4 : 1.0\n",
            "                last_two = 'la'           female : male   =     69.7 : 1.0\n",
            "             last_letter = 'a'            female : male   =     37.2 : 1.0\n",
            "                last_two = 'sa'           female : male   =     34.4 : 1.0\n",
            "             last_letter = 'k'              male : female =     30.1 : 1.0\n",
            "                last_two = 'rd'             male : female =     30.0 : 1.0\n",
            "                last_two = 'ta'           female : male   =     29.5 : 1.0\n",
            "                last_two = 'us'             male : female =     27.6 : 1.0\n",
            "              last_three = 'ana'          female : male   =     24.6 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "pKapTDqxfkRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict gender\n",
        "def predict_gender(name, classifier):\n",
        "    return classifier.classify(gender_features(name))\n",
        "# Test the classifiers with sample names\n",
        "sample_names = [\"Alice\", \"John\", \"Taylor\", \"Jordan\", \"Sam\"]\n",
        "for name in sample_names:\n",
        "    print(f\"\\nName: {name}\")\n",
        "    print(f\"  Naive Bayes: {predict_gender(name, nb_classifier)}\")\n",
        "    print(f\"  Decision Tree: {predict_gender(name, dt_classifier)}\")\n",
        "    print(f\"  MaxEnt: {predict_gender(name, me_classifier)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-zRRS1lhrcU",
        "outputId": "a044a193-72d6-4987-c1c7-b9d221a837d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Name: Alice\n",
            "  Naive Bayes: female\n",
            "  Decision Tree: female\n",
            "  MaxEnt: female\n",
            "\n",
            "Name: John\n",
            "  Naive Bayes: male\n",
            "  Decision Tree: male\n",
            "  MaxEnt: male\n",
            "\n",
            "Name: Taylor\n",
            "  Naive Bayes: male\n",
            "  Decision Tree: male\n",
            "  MaxEnt: male\n",
            "\n",
            "Name: Jordan\n",
            "  Naive Bayes: male\n",
            "  Decision Tree: female\n",
            "  MaxEnt: male\n",
            "\n",
            "Name: Sam\n",
            "  Naive Bayes: male\n",
            "  Decision Tree: male\n",
            "  MaxEnt: male\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comparison of Test Set vs. Dev-Test Set Performance**\n",
        "\n",
        "The Naive Bayes and MaxEnt classifiers show only a slight drop in accuracy from the dev-test set to the test set, whereas the Decision Tree classifier maintains almost the same accuracy.\n",
        "\n",
        "**Naive Bayes:**\n",
        "\n",
        "- Dev-Test Accuracy: 82.0%\n",
        "\n",
        "- Test Accuracy: 80.6%\n",
        "\n",
        "- Drop: 1.4%\n",
        "\n",
        "**MaxEnt (Maximum Entropy):**\n",
        "\n",
        "- Dev-Test Accuracy: 81.8%\n",
        "\n",
        "- Test Accuracy: 81.0%\n",
        "\n",
        "- Drop: 0.8%\n",
        "\n",
        "\n",
        "**Decision Tree:**\n",
        "\n",
        "- Dev-Test Accuracy: 73.6%\n",
        "\n",
        "- Test Accuracy: 73.8%\n",
        "\n",
        "- Increase: +0.2%"
      ],
      "metadata": {
        "id": "fCZm_V0RlqN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Is This Expected?**\n",
        "\n",
        "Yes, this is generally expected because:\n",
        "\n",
        "**Slight Accuracy Drop (NB & MaxEnt)**\n",
        "\n",
        "The slight decrease in accuracy for Naive Bayes and MaxEnt indicates that the dev-test set was representative of the test set, meaning the models generalized well.\n",
        "\n",
        "A small accuracy drop is normal due to slight variations in data distribution between dev-test and test sets.\n",
        "\n",
        "**Decision Tree's Stability**\n",
        "\n",
        "The Decision Tree classifier performed similarly on both the dev-test and test sets. This suggests it was already overfitting on the training set and did not improve significantly with new data.\n",
        "Decision Trees tend to memorize patterns rather than generalizing well.\n",
        "Feature Effectiveness\n",
        "\n",
        "The most informative features (last two letters, last letter, etc.) align well with common naming conventions, which is why Naive Bayes and MaxEnt performed better."
      ],
      "metadata": {
        "id": "K1nxaUs8mjAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusion**\n",
        "\n",
        "Naive Bayes and MaxEnt are the best choices for this task because they generalize well.\n",
        "\n",
        "Decision Trees overfit easily, which is why it struggled.\n",
        "\n",
        "The small drop in accuracy is expected, showing that the dev-test set was a good predictor of real performance.\n",
        "If you want to improve further, we could:\n",
        "\n",
        "- Add more features (e.g., syllables, consonant-vowel patterns).\n",
        "\n",
        "- Use an ensemble model (combine NB + MaxEnt for better results).\n",
        "\n",
        "- Train on a larger dataset to capture more name variations."
      ],
      "metadata": {
        "id": "tm-XbhkhnBzi"
      }
    }
  ]
}